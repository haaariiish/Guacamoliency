{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f98b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabakaran/miniconda3/envs/train_2_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d04363",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../data/tokenizers_character_level/moses_ClearSMILES_corrected\")\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8a09df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 8,\n",
       " '2': 9,\n",
       " ')': 0,\n",
       " '<unk>': 11,\n",
       " 'S': 7,\n",
       " 'C': 5,\n",
       " 'Cl': 15,\n",
       " '1': 16,\n",
       " 'O': 6,\n",
       " '=': 13,\n",
       " '<pad>': 12,\n",
       " 'F': 14,\n",
       " '<eos>': 4,\n",
       " '<bos>': 3,\n",
       " 'Br': 10,\n",
       " '#': 2,\n",
       " 'N': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34729ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: '<unk>',\n",
       " 4: '<eos>',\n",
       " 5: 'C',\n",
       " 12: '<pad>',\n",
       " 13: '=',\n",
       " 8: '(',\n",
       " 7: 'S',\n",
       " 3: '<bos>',\n",
       " 16: '1',\n",
       " 9: '2',\n",
       " 1: 'N',\n",
       " 6: 'O',\n",
       " 15: 'Cl',\n",
       " 14: 'F',\n",
       " 2: '#',\n",
       " 10: 'Br',\n",
       " 0: ')'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abvoc = {}\n",
    "for k in vocab.keys():\n",
    "    abvoc[vocab[k]] = k\n",
    "abvoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "moses_dataset = pd.read_csv(\"../data/training_data/moses_ClearSMILES.csv\")\n",
    "#D = len(moses_dataset[moses_dataset[\"SPLIT\"] ==\"train\"][\"SMILES\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff51176",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf =[0]*len(tokenizer) \n",
    "D = len(moses_dataset[\"SMILES\"])\n",
    "for k in vocab.keys():\n",
    "    for l in moses_dataset[\"SMILES\"]:\n",
    "        if k in l:\n",
    "            idf[vocab[k]] +=1\n",
    "idf = [np.log(D/k) if k!=0 else 0 for k in idf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc551f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0031287468135231685,\n",
       " 0.009568700773703875,\n",
       " 2.5602130118820057,\n",
       " 0,\n",
       " 0,\n",
       " 0.0,\n",
       " 0.0357366804309882,\n",
       " 1.1437764470791782,\n",
       " 0.0031287468135231685,\n",
       " 1.0860727903801972,\n",
       " 3.413334466828707,\n",
       " 0,\n",
       " 0,\n",
       " 0.00017998683182921782,\n",
       " 1.666313076800451,\n",
       " 2.249299868646848,\n",
       " 0.00038393511170831525]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29b42d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 11,\n",
       " '<eos>': 4,\n",
       " 'C': 5,\n",
       " '<pad>': 12,\n",
       " '=': 13,\n",
       " '(': 8,\n",
       " 'S': 7,\n",
       " '<bos>': 3,\n",
       " '1': 16,\n",
       " '2': 9,\n",
       " 'N': 1,\n",
       " 'O': 6,\n",
       " 'Cl': 15,\n",
       " 'F': 14,\n",
       " '#': 2,\n",
       " 'Br': 10,\n",
       " ')': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690515a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e9ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 69.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(11.3874, dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"../models/trained_moses_ClearSMILES_corrected_character_level/2/final_model\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "loss_list = []\n",
    "weights = [ idf[k] for k in range(len(idf))]\n",
    "criterion_weighted = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
    "for k in tqdm(moses_dataset[\"SMILES\"][1000:2000]):\n",
    "    x = tokenizer(k,return_tensors='pt')\n",
    "    inputs = x[\"input_ids\"].squeeze()\n",
    "    model = model.double()\n",
    "\n",
    "    y = model(x[\"input_ids\"].squeeze()).logits\n",
    "\n",
    "    loss_weighted = criterion_weighted(y,inputs)\n",
    "    loss_list.append(loss_weighted)\n",
    "m = 0\n",
    "for k in loss_list:\n",
    "    m+=k/len(loss_list)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"../models/trained_moses_ClearSMILES_corrected_character_level/2/final_model\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "loss_list = []\n",
    "\n",
    "criterion_weighted = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4dbe20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = moses_dataset[\"SMILES\"][0]\n",
    "x = tokenizer(k,return_tensors='pt')\n",
    "inputs = x[\"input_ids\"].squeeze()\n",
    "model = model.double()\n",
    "\n",
    "y = model(x[\"input_ids\"].squeeze()).logits\n",
    "\n",
    "loss_weighted = criterion_weighted(y,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa2f6988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 3, 14,  5, 16, 13,  5,  5, 13,  5,  8, 14,  0,  5, 13,  5, 16,  1,  7,\n",
       "          8, 13,  6,  0,  8, 13,  6,  0,  5, 16, 13,  5,  5, 13,  5,  8, 14,  0,\n",
       "          5,  8, 14,  0, 13,  5, 16, 14,  4]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7b79bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab74b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[l].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "aa1ad1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9295, dtype=torch.float64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss = criterion_weighted(true_y.double(), y.double())\n",
    "min_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = moses_dataset[\"SMILES\"][0]\n",
    "x = tokenizer(k,return_tensors='pt')\n",
    "inputs = x[\"input_ids\"].squeeze()\n",
    "y = model(x[\"input_ids\"].squeeze()).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "918c0baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.8270, dtype=torch.float64, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = moses_dataset[\"SMILES\"][0]\n",
    "x = tokenizer(k,return_tensors='pt')\n",
    "inputs = x[\"input_ids\"].squeeze()\n",
    "y = model(x[\"input_ids\"].squeeze()).logits\n",
    "weights = [ idf[k] for k in range(len(idf))]\n",
    "nn.functional.cross_entropy(y.double(),target=inputs,weight=torch.tensor(weights),reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ab9a3060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9295, dtype=torch.float64)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.cross_entropy(true_y.double(),target=y.double(),reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8fef7733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9678, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = nn.functional.cross_entropy(input, target)\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a811057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9678, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65e9c697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(17, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config =   GPT2Config(\n",
    "            vocab_size=len(tokenizer),  # 10,000 tokens( pour BEP )\n",
    "            n_positions=tokenizer.model_max_length , # ça ne génèrera que des smiles de la même taille\n",
    "            n_ctx=tokenizer.model_max_length,  # ça ne génèrera que des smiles de la même taille\n",
    "            n_embd=256,\n",
    "            n_layer=8,\n",
    "            n_head=8,\n",
    "            resid_pdrop=0.1,\n",
    "            embd_pdrop=0.1,\n",
    "            attn_pdrop=0.1\n",
    "        )\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d8aa261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# ==================== OPTION 1: Classe Loss personnalisée ====================\n",
    "class WeightedLanguageModelLoss(nn.Module):\n",
    "    def __init__(self, vocab_weights=None, ignore_index=-100, label_smoothing=0.0):\n",
    "        \"\"\"\n",
    "        Loss function pour un language model avec poids personnalisés\n",
    "        \n",
    "        Args:\n",
    "            vocab_weights: Tensor de poids pour chaque token du vocabulaire [vocab_size]\n",
    "            ignore_index: Index à ignorer dans le calcul (padding tokens)\n",
    "            label_smoothing: Lissage des labels (0.0 = pas de lissage)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_weights = vocab_weights\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: [batch_size, seq_len, vocab_size] ou [batch_size*seq_len, vocab_size]\n",
    "            labels: [batch_size, seq_len] ou [batch_size*seq_len]\n",
    "        \"\"\"\n",
    "        # Si les logits ont 3 dimensions, on fait le shift pour LM\n",
    "        if logits.dim() == 3:\n",
    "            # Shift pour prédiction du token suivant\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Reshape pour cross_entropy\n",
    "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "        else:\n",
    "            shift_logits = logits\n",
    "            shift_labels = labels\n",
    "        \n",
    "        # Calcul de la cross entropy\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits,\n",
    "            shift_labels,\n",
    "            weight=self.vocab_weights,\n",
    "            ignore_index=self.ignore_index,\n",
    "            label_smoothing=self.label_smoothing,\n",
    "            reduction='mean'\n",
    "        )\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# ==================== OPTION 2: Fonction Loss simple ====================\n",
    "def compute_language_model_loss(model_output, input_ids, vocab_weights=None):\n",
    "    \"\"\"\n",
    "    Fonction simple pour calculer la loss d'un language model\n",
    "    \"\"\"\n",
    "    logits = model_output.logits  # [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "    # Shift pour prédiction du token suivant\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = input_ids[..., 1:].contiguous()\n",
    "    \n",
    "    # Reshape\n",
    "    shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    \n",
    "    # Cross entropy\n",
    "    loss = F.cross_entropy(\n",
    "        shift_logits,\n",
    "        shift_labels,\n",
    "        weight=vocab_weights,\n",
    "        reduction='mean'\n",
    "    )\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# ==================== EXEMPLE D'USAGE DANS UNE BOUCLE D'ENTRAÎNEMENT ====================\n",
    "\n",
    "# Préparation des poids IDF\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "idf_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "# Option 1: Avec classe Loss\n",
    "criterion = WeightedLanguageModelLoss(vocab_weights=idf_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3f2544aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y = []\n",
    "for k in range(len(inputs)):\n",
    "    mmmm = [1]*len(tokenizer)\n",
    "    mmmm[inputs[k]] = 5\n",
    "    true_y.append(mmmm)\n",
    "true_y = torch.tensor(true_y)\n",
    "\n",
    "y = []\n",
    "for k in range(len(inputs)):\n",
    "    mmmm = [0]*len(tokenizer)\n",
    "    mmmm[inputs[k]] = 9000\n",
    "    y.append(mmmm)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "cc6296c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(y.float(), inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b655f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2570)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(true_y.float(), inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277ce77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
